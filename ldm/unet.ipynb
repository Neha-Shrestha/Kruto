{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_conv(ic, oc, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ic))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ic, oc, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(ic, oc, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ic))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ic, oc, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(tsteps, emb_dim, max_period= 10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ic, nheads, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ic//nheads\n",
    "        self.scale = math.sqrt(ic/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ic)\n",
    "        self.qkv = nn.Linear(ic, ic*3)\n",
    "        self.proj = nn.Linear(ic, ic)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1) @ v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ic, oc=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.emb_proj = nn.Linear(n_emb, oc*2)\n",
    "        self.conv1 = unet_conv(ic, oc, ks, act=act, norm=norm)\n",
    "        self.conv2 = unet_conv(oc, oc, ks, act=act, norm=norm)\n",
    "        self.idconv = nn.Identity() if ic==oc else nn.Conv2d(ic, oc, kernel_size=1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttentionMultiHead(oc, attn_chans)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(act(t))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x) + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Encoder(nn.Module):\n",
    "    def __init__(self, n_emb, in_channels, nf, attn_chans):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        for f in nf:\n",
    "            self.encoder.append(ResBlock(n_emb, in_channels, oc=f, attn_chans=attn_chans))\n",
    "            in_channels = f\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        skips = []\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x, t)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Decoder(nn.Module):\n",
    "    def __init__(self, n_emb, nf, attn_chans):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for f in nf:\n",
    "            self.decoder.append(nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2))\n",
    "            self.decoder.append(ResBlock(n_emb, f*2, oc=f, attn_chans=attn_chans))\n",
    "    \n",
    "    def forward(self, x, skips, t):\n",
    "        for i in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[i](x)\n",
    "            x = torch.cat((skips[i//2], x), dim=1)\n",
    "            x = self.decoder[i+1](x, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_dummy(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels, out_channels, nf=[64, 128, 256, 512], attn_chans=8):\n",
    "        super().__init__()\n",
    "        self.t_emb = nf[0]\n",
    "        n_emb = self.t_emb*4\n",
    "        self.cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "\n",
    "        self.emb_mlp = nn.Sequential(\n",
    "            lin(self.t_emb, n_emb, act=None, norm=nn.BatchNorm1d),\n",
    "            lin(n_emb, n_emb)\n",
    "        )\n",
    "\n",
    "        self.unet_encoder = UNET_Encoder(n_emb, in_channels, nf, attn_chans)\n",
    "\n",
    "        self.bottle_neck = ResBlock(n_emb, nf[-1], nf[-1])\n",
    "\n",
    "        self.unet_decoder = UNET_Decoder(n_emb, nf[::-1], attn_chans)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(nf[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x, t, c = inp\n",
    "        temb = timestep_embedding(t, self.t_emb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "        x, skips = self.unet_encoder(x, emb)\n",
    "        x = self.bottle_neck(x, emb)\n",
    "        x = self.unet_decoder(x, skips[::-1], emb)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET_dummy(\n",
       "  (cond_emb): Embedding(10, 256)\n",
       "  (emb_mlp): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (unet_encoder): UNET_Encoder(\n",
       "    (encoder): ModuleList(\n",
       "      (0): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bottle_neck): ResBlock(\n",
       "    (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (conv1): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (idconv): Identity()\n",
       "  )\n",
       "  (unet_decoder): UNET_Decoder(\n",
       "    (decoder): ModuleList(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (5): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (7): ResBlock(\n",
       "        (emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (conv1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (attn): SelfAttentionMultiHead(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud = UNET_dummy(10, 1, 1)\n",
    "ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels, out_channels, nf=[64, 128, 256, 512], attn_chans=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.t_emb = nf[0]\n",
    "        n_emb = self.t_emb*4\n",
    "        self.cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "\n",
    "        self.emb_mlp = nn.Sequential(\n",
    "            lin(self.t_emb, n_emb, act=None, norm=nn.BatchNorm1d),\n",
    "            lin(n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "        for f in nf:\n",
    "            self.encoder.append(ResBlock(n_emb, in_channels, oc=f, attn_chans=attn_chans))\n",
    "            in_channels = f\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottle_neck = ResBlock(n_emb, nf[-1], nf[-1])\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        rnf = nf[::-1]\n",
    "        for f in rnf:\n",
    "            self.decoder.append(nn.ConvTranspose2d(f*2, f, kernel_size=2, stride=2))\n",
    "            self.decoder.append(ResBlock(n_emb, f*2, oc=f, attn_chans=attn_chans))\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(nf[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x, t, c = inp\n",
    "        temb = timestep_embedding(t, self.t_emb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "\n",
    "        skips = []\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x, emb)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "              \n",
    "        x = self.bottle_neck(x)\n",
    "        skips = skips[::-1]\n",
    "        \n",
    "        for i in range(0, len(self.decoder), 2):\n",
    "            x = self.decoder[i](x)\n",
    "            x = torch.cat((skips[i//2], x), dim=1)\n",
    "            x = self.decoder[i+1](x, emb)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (encoder): ModuleList(\n",
       "    (0): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cond_emb): Embedding(10, 256)\n",
       "  (emb_mlp): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottle_neck): ResBlock(\n",
       "    (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (conv1): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (idconv): Identity()\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): ResBlock(\n",
       "      (emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (conv1): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (attn): SelfAttentionMultiHead(\n",
       "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNET(10, 1, 1)\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
