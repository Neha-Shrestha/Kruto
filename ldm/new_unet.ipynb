{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n",
    "from learner import *\n",
    "from time_embedding import *\n",
    "from einops import rearrange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import init_attr\n",
    "\n",
    "class DDPM:\n",
    "    def __init__(self, beta_min, beta_max, n_steps):\n",
    "        init_attr(self, locals=locals())\n",
    "        self.beta = torch.linspace(self.beta_min, self.beta_max, self.n_steps)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = self.alpha.cumprod(dim=0)\n",
    "        self.sigma = self.beta.sqrt()\n",
    "    \n",
    "    def schedule(self, x0):\n",
    "        device = \"cuda\"\n",
    "        n = len(x0)\n",
    "        t = torch.randint(0, self.n_steps, (n,), dtype=torch.long)\n",
    "        noise = torch.randn(x0.shape)\n",
    "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1).to(device)\n",
    "        mean = alpha_bar_t.sqrt().to(device) * x0.to(device) \n",
    "        variance = (1-alpha_bar_t).sqrt().to(device) * noise.to(device) \n",
    "        xt = mean + variance\n",
    "        return (xt, t.to(device)), noise.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_conv(in_channels, out_channels, kernel_size=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(in_channels))\n",
    "    if act: layers.append(act())\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestepEmbedding(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_embedding = self.n_channels * 4\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(self.n_channels, self.n_embedding),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.n_embedding, self.n_embedding)\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        half_dim = self.n_embedding // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None].float() * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "        emb = self.timestep_mlp(emb)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_embedding, in_channels, out_channels=None, kernel_size=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_channs=0):\n",
    "        super().__init__()\n",
    "        if out_channels is None: out_channels = in_channels\n",
    "        self.emb_proj = nn.Linear(n_embedding, out_channels*2)\n",
    "        self.conv_1 = unet_conv(in_channels, out_channels, kernel_size=kernel_size, act=act, norm=norm)\n",
    "        self.conv_2 = unet_conv(out_channels, out_channels, kernel_size=kernel_size, act=act, norm=norm)\n",
    "        if in_channels == out_channels:\n",
    "            self.id_conv = nn.Identity()  \n",
    "        else:\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.attn = False\n",
    "        if attn_channs:\n",
    "            self.attn = SelfAttention2D(out_channels, attn_channs)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv_1(x)\n",
    "        # x += self.emb_proj(act(t))[:, :, None, None]\n",
    "        emb = self.emb_proj(act(t))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x) + self.idconv(inp)\n",
    "        if self.attn:\n",
    "            x += self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Encoder(nn.Module):\n",
    "    def __init__(self, n_embedding, channels, attn_channs=0, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.down = nn.ModuleList()\n",
    "        self.down_sample = nn.ModuleList()\n",
    "        \n",
    "        n_resolutions = len(channels)\n",
    "        out_channels = channels[0]\n",
    "        for i in range(n_resolutions):\n",
    "            in_channels = out_channels\n",
    "            out_channels = channels[i]\n",
    "            for i in range(2):\n",
    "                self.down.append(\n",
    "                    ResBlock(\n",
    "                        n_embedding, \n",
    "                        in_channels if i==0 else out_channels, \n",
    "                        out_channels=out_channels, \n",
    "                        attn_channs=0 if i<attn_start else attn_channs\n",
    "                    )\n",
    "                )\n",
    "            self.down_sample.append(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1) if i < (n_resolutions-1) else nn.Identity()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        skips = []\n",
    "        for i in range(len(self.down)):\n",
    "            x = self.down[i](x, t)\n",
    "            skips.append(x)\n",
    "            x = self.down_sample[i](x)\n",
    "        return x, skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_encoder = UNET_Encoder(256, (32, 64, 128, 256), attn_channs=8)\n",
    "# unet_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Bottleneck(nn.Module):\n",
    "    def __init__(self, n_embedding, in_channels):\n",
    "        super().__init__()\n",
    "        self.unet_bottleneck = nn.Sequential(\n",
    "            ResBlock(n_embedding, in_channels, attn_channs=8),\n",
    "            ResBlock(n_embedding, in_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.unet_bottleneck(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_bottleneck = UNET_Bottleneck(256, 256)\n",
    "# unet_bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Decoder(nn.Module):\n",
    "    def __init__(self, n_embedding, channels, attn_channs=0, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.up = nn.ModuleList()\n",
    "        self.up_sample = nn.ModuleList()\n",
    "\n",
    "        n_resolutions = len(channels)\n",
    "        out_channels = channels[0]\n",
    "        for i in range(n_resolutions):\n",
    "            prev_channels = out_channels\n",
    "            in_channels = channels[min(i+1, n_resolutions-1)]\n",
    "            out_channels = channels[i]\n",
    "            for i in range(3):\n",
    "                self.up.append(\n",
    "                    ResBlock(\n",
    "                        n_embedding, \n",
    "                        (prev_channels if i==0 else out_channels) + (in_channels if i==2 else out_channels), \n",
    "                        out_channels=out_channels, \n",
    "                        attn_channs=0 if i>=n_resolutions-attn_start else attn_channs\n",
    "                    )\n",
    "                )\n",
    "            self.up_sample.append(\n",
    "                nn.ConvTranspose2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1) if i < (n_resolutions-1) else nn.Identity()\n",
    "            )\n",
    "\n",
    "    def forward(self, x, t, skips):\n",
    "        for i in range(len(self.up)):\n",
    "            x = self.up_sample[i](x)\n",
    "            x = torch.cat((skips[i], x), dim=1)\n",
    "            x = self.up[i](x, t)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_decoder = UNET_Decoder(256, (256, 128, 64, 32), attn_channs=8)\n",
    "# unet_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels, out_channels, channels=(64, 128, 256, 512), attn_channs=8):\n",
    "        super().__init__()\n",
    "        self.n_embedding = channels[0]*4\n",
    "        self.timestep_embedding = TimestepEmbedding(channels[0])\n",
    "        self.condition_embedding = nn.Embedding(n_classes, self.n_embedding)\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, channels[0], kernel_size=3, padding=1)\n",
    "        self.encoder = UNET_Encoder(self.n_embedding, channels, attn_channs=attn_channs)\n",
    "        self.bottleneck = UNET_Bottleneck(self.n_embedding, channels[-1])\n",
    "        self.decoder = UNET_Decoder(self.n_embedding, channels[::-1], attn_channs=attn_channs)\n",
    "        self.conv_out = unet_conv(channels[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x, t, c = inp\n",
    "        t = self.timestep_embedding(t)\n",
    "        c = self.condition_embedding(c)\n",
    "        emb = t + c\n",
    "        x = self.conv_in(x)\n",
    "        x, skips = self.encoder(x, emb)\n",
    "        x = self.bottleneck(x, emb)\n",
    "        x = self.decoder(x, emb, skips[::-1])\n",
    "        x = self.conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (timestep_embedding): TimestepEmbedding(\n",
       "    (timestep_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (condition_embedding): Embedding(10, 128)\n",
       "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder): UNET_Encoder(\n",
       "    (down): ModuleList(\n",
       "      (0): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down_sample): ModuleList(\n",
       "      (0-1): 2 x Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): UNET_Bottleneck(\n",
       "    (unet_bottleneck): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UNET_Decoder(\n",
       "    (up): ModuleList(\n",
       "      (0-1): 2 x ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_sample): ModuleList(\n",
       "      (0): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (2-3): 2 x ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (conv_out): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNET(10, 1, 1, (32,64,128,256))\n",
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
