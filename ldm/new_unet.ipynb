{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n",
    "from learner import *\n",
    "from time_embedding import *\n",
    "from einops import rearrange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_conv(in_channels, out_channels, kernel_size=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(in_channels))\n",
    "    if act: layers.append(act())\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_embedding, in_channels, out_channels=None, kernel_size=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_channs=0):\n",
    "        super().__init__()\n",
    "        if out_channels is None: out_channels = in_channels\n",
    "        self.emb_proj = nn.Linear(n_embedding, out_channels*2)\n",
    "        self.conv_1 = unet_conv(in_channels, out_channels, kernel_size=kernel_size, act=act, norm=norm)\n",
    "        self.conv_2 = unet_conv(out_channels, out_channels, kernel_size=kernel_size, act=act, norm=norm)\n",
    "        self.id_conv = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.attn = False\n",
    "        # if attn_channs: self.attn = SelfAttention2D(out_channels, attn_channs)\n",
    "        if attn_channs: self.attn = SelfAttention(out_channels, attn_channs)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv_1(x)\n",
    "        # x += self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv_2(x) \n",
    "        x += self.id_conv(inp)\n",
    "        if self.attn:\n",
    "            x += self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, kernel_size=3, stride=2, padding=1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Encoder(nn.Module):\n",
    "    def __init__(self, n_embedding, channels, attn_channs=0, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.down_sample = nn.ModuleList()\n",
    "        \n",
    "        n_resolutions = len(channels)\n",
    "        out_channels = channels[0]\n",
    "        for i in range(n_resolutions):\n",
    "            in_channels = out_channels\n",
    "            out_channels = channels[i]\n",
    "            down = nn.ModuleList()\n",
    "            for j in range(2):\n",
    "                down.append(\n",
    "                    ResBlock(\n",
    "                        n_embedding, \n",
    "                        in_channels if j==0 else out_channels, \n",
    "                        out_channels=out_channels, \n",
    "                        attn_channs=0 if j<attn_start else attn_channs\n",
    "                    )\n",
    "                )\n",
    "            self.down_blocks.append(down)\n",
    "            self.down_sample.append(\n",
    "                DownSample(out_channels) if (i < n_resolutions-1) and (j == 1) else nn.Identity()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        skips = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            for down in self.down_blocks[i]:\n",
    "                skips.append(x)\n",
    "                x = down(x, t)\n",
    "            skips.append(x)\n",
    "            x = self.down_sample[i](x)\n",
    "        return x, skips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Bottleneck(nn.Module):\n",
    "    def __init__(self, n_embedding, in_channels):\n",
    "        super().__init__()\n",
    "        self.unet_bottleneck_1 = ResBlock(n_embedding, in_channels, attn_channs=8)\n",
    "        self.unet_bottleneck_2 = ResBlock(n_embedding, in_channels)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        x = self.unet_bottleneck_1(x, t)\n",
    "        return self.unet_bottleneck_2(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2.0),\n",
    "            nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_Decoder(nn.Module):\n",
    "    def __init__(self, n_embedding, channels, attn_channs=0, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.up_sample = nn.ModuleList()\n",
    "\n",
    "        n_resolutions = len(channels)\n",
    "        out_channels = channels[0]\n",
    "        for i in range(n_resolutions):\n",
    "            prev_channels = out_channels\n",
    "            in_channels = channels[min(i+1, n_resolutions-1)]\n",
    "            out_channels = channels[i]\n",
    "            up = nn.ModuleList()\n",
    "            for j in range(3):\n",
    "                up.append(\n",
    "                    ResBlock(\n",
    "                        n_embedding, \n",
    "                        (prev_channels if j==0 else out_channels) + (in_channels if j==2 else out_channels), \n",
    "                        out_channels=out_channels, \n",
    "                        attn_channs=0 if j>=n_resolutions-attn_start else attn_channs\n",
    "                    )\n",
    "                )\n",
    "            self.up_blocks.append(up)\n",
    "            self.up_sample.append(\n",
    "                UpSample(out_channels) if (i < n_resolutions-1) and (j==2) else nn.Identity()\n",
    "            )\n",
    "\n",
    "    def forward(self, x, t, skips):\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            for up in self.up_blocks[i]:\n",
    "                x = up(torch.cat((x, skips.pop()), dim=1), t)\n",
    "            x = self.up_sample[i](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, n_classes, in_channels, out_channels, channels=(64, 128, 256, 512), attn_channs=8):\n",
    "        super().__init__()\n",
    "        self.n_channels = channels[0]\n",
    "        self.n_embedding = self.n_channels * 4\n",
    "        self.timestep_embedding = TimestepEmbedding(self.n_channels, self.n_embedding)\n",
    "        self.condition_embedding = nn.Embedding(n_classes, self.n_embedding)\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, channels[0], kernel_size=3, padding=1)\n",
    "        self.encoder = UNET_Encoder(self.n_embedding, channels, attn_channs=attn_channs)\n",
    "        self.bottleneck = UNET_Bottleneck(self.n_embedding, channels[-1])\n",
    "        self.decoder = UNET_Decoder(self.n_embedding, channels[::-1], attn_channs=attn_channs)\n",
    "        self.conv_out = unet_conv(channels[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x, t, c = inp\n",
    "        emb = self.timestep_embedding(t) + self.condition_embedding(c)\n",
    "        x = self.conv_in(x)\n",
    "        x, skips = self.encoder(x, emb)\n",
    "        x = self.bottleneck(x, emb)\n",
    "        x = self.decoder(x, emb, skips)\n",
    "        x = self.conv_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (timestep_embedding): TimestepEmbedding(\n",
       "    (timestep_mlp): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (condition_embedding): Embedding(10, 128)\n",
       "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (encoder): UNET_Encoder(\n",
       "    (down): ModuleList(\n",
       "      (0): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down_sample): ModuleList(\n",
       "      (0): DownSample(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (1): DownSample(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (2): DownSample(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (3): Identity()\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): UNET_Bottleneck(\n",
       "    (unet_bottleneck): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (id_conv): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UNET_Decoder(\n",
       "    (up): ModuleList(\n",
       "      (0-1): 2 x ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ResBlock(\n",
       "        (emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (conv_1): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (conv_2): Sequential(\n",
       "          (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): SiLU()\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (attn): SelfAttention2D(\n",
       "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv): Linear(in_features=32, out_features=96, bias=True)\n",
       "          (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_sample): ModuleList(\n",
       "      (0): UpSample(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): UpSample(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): UpSample(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): Identity()\n",
       "    )\n",
       "  )\n",
       "  (conv_out): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNET(10, 1, 1, (32,64,128,256))\n",
    "unet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
